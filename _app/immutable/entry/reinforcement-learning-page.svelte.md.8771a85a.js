import{S as wr,i as yr,s as Ar,U as ni,y as xr,z as br,A as Ir,V as Rr,W as hr,g as Sr,d as Tr,B as kr,X as _r,k as i,q as n,a as u,Y as Cr,l as r,m as a,r as s,h as t,c as d,Z as Pr,n as v,b as f,D as l,E as zr}from"../chunks/index.34fbf63c.js";import{P as Hr}from"../chunks/post_layout.004ae77b.js";import{k as Lr}from"../chunks/posts.d122aa8c.js";function pr(p,c){c={wrap:!1,throwOnError:!1,...c};const h=c.wrap?p:`{${p}}`;return Lr.renderToString(h,c)}function Mr(p,c){c={overflowAuto:!0,throwOnError:!1,displayMode:!0,...c};let h=Lr.renderToString(p,c);return h=c.overflowAuto?`<div style="overflow-x:auto;" class="mathlifier-display">${h}</div>`:h,h}function qr(p){let c,h,x,L,m,_,j,tt,Z,Pt,lt,P,W,zt,it,g,be,Ht,Mt,Ie,qt,Ut,Re,Nt,rt,z,X,Bt,at,R,Se,Dt,$t,Te,Ot,ot,H,Y,Vt,nt,J,ke,Gt,st,M,K,Ft,ft,E,Q,Ce,jt,Zt,Wt,Pe,Xt,Yt,ze,Jt,Kt,He,Qt,ut,q,ee,el,dt,te,le,tl,U,ie,ll,ct,il,Me,vt,N,re,rl,mt,w,qe,al,ol,Ue,nl,sl,Ne,fl,ht,B,ae,ul,_t,y,Be,dl,cl,De,vl,ml,$e,hl,pt,D,oe,_l,Et,A,ne,Oe,pl,El,Ll,se,Ve,gl,wl,yl,fe,Ge,Al,xl,Lt,$,ue,bl,gt,S,de,Fe,Il,Rl,Sl,ce,je,Tl,kl,wt,O,ve,Cl,yt,V,me,Pl,At,T,he,zl,b,Ze,Hl,Ml,We,ql,Ul,Xe,Nl,Bl,_e,Dl,I,Ye,$l,Ol,Je,Vl,Gl,Ke,Fl,xt,G,pe,jl,bt,F,Ee,Zl,It,k,Qe,Wl,Xl,et,Yl;return{c(){c=i("h1"),h=i("a"),x=n("Reinforcement Learning Course Notes: David Silver’s Course"),L=u(),m=i("h1"),_=i("a"),j=n("Lecture 1: Introduction to RL"),tt=u(),Z=i("p"),Pt=n("Created: May 6, 2023 9:27 PM"),lt=u(),P=i("h3"),W=i("a"),zt=n("Branches of Machine learning:"),it=u(),g=i("ul"),be=i("li"),Ht=n("Supervised Learning"),Mt=u(),Ie=i("li"),qt=n("Unsupervised Learning"),Ut=u(),Re=i("li"),Nt=n("Reinforcement Learning"),rt=u(),z=i("h3"),X=i("a"),Bt=n("Rewards:"),at=u(),R=i("ul"),Se=i("li"),Dt=n("A reward is a scalar feedback signal"),$t=u(),Te=i("li"),Ot=n("Agent’s job is to maximize cumulative reward"),ot=u(),H=i("h3"),Y=i("a"),Vt=n("Reward Hypothesis:"),nt=u(),J=i("ul"),ke=i("li"),Gt=n("All goals cam be described by the maximization of expected cumulative reward"),st=u(),M=i("h3"),K=i("a"),Ft=n("Sequential Decision making:"),ft=u(),E=i("ul"),Q=i("li"),Ce=i("strong"),jt=n("Goal:"),Zt=n("  Select actions to maximize total future reward"),Wt=u(),Pe=i("li"),Xt=n("Actions may have long term consequences"),Yt=u(),ze=i("li"),Jt=n("Reward may be delayed"),Kt=u(),He=i("li"),Qt=n("It may be better to sacrifice immediate reward to gain more long-term reward"),ut=u(),q=i("h3"),ee=i("a"),el=n("Agent and Environment:"),dt=u(),te=i("ul"),le=i("li"),tl=n("At Each step t, the agent:"),U=i("ul"),ie=i("li"),ll=n("Executes an action "),ct=new Cr(!1),il=u(),Me=i("li"),vt=u(),N=i("h3"),re=i("a"),rl=n("Value Function:"),mt=u(),w=i("ul"),qe=i("li"),al=n("Prediction of future reward"),ol=u(),Ue=i("li"),nl=n("used to evaluate the goodness/badness of state"),sl=u(),Ne=i("li"),fl=n("select between actions"),ht=u(),B=i("h3"),ae=i("a"),ul=n("Model:"),_t=u(),y=i("ul"),Be=i("li"),dl=n("A model predicts what environment will do next"),cl=u(),De=i("li"),vl=n("P predicts the next state"),ml=u(),$e=i("li"),hl=n("R predicts the reward"),pt=u(),D=i("h3"),oe=i("a"),_l=n("Categorizing RL Agents (1):"),Et=u(),A=i("ul"),ne=i("li"),Oe=i("strong"),pl=n("Value Function:"),El=n("  No policy but value function"),Ll=u(),se=i("li"),Ve=i("strong"),gl=n("Policy Based:"),wl=n("  No value function but policy"),yl=u(),fe=i("li"),Ge=i("strong"),Al=n("Actor-Critic:"),xl=n(" Both policy and value function"),Lt=u(),$=i("h3"),ue=i("a"),bl=n("Categorizing RL Agents (2):"),gt=u(),S=i("ul"),de=i("li"),Fe=i("strong"),Il=n("Model-Free:"),Rl=n(" Policy and/or Value function"),Sl=u(),ce=i("li"),je=i("strong"),Tl=n("Model-Based:"),kl=n(" Policy and/or Value function and Model"),wt=u(),O=i("h3"),ve=i("a"),Cl=n("RL Agent Taxonomy:"),yt=u(),V=i("h3"),me=i("a"),Pl=n("Two fundamental problems in Sequential Decision making:"),At=u(),T=i("ul"),he=i("li"),zl=n("RL :"),b=i("ul"),Ze=i("li"),Hl=n("The environment is initially unknown"),Ml=u(),We=i("li"),ql=n("The Agent interacts with environment"),Ul=u(),Xe=i("li"),Nl=n("The Agent improves its policy"),Bl=u(),_e=i("li"),Dl=n("Planning:"),I=i("ul"),Ye=i("li"),$l=n("A model of env. is known"),Ol=u(),Je=i("li"),Vl=n("Agent performs computation with model"),Gl=u(),Ke=i("li"),Fl=n("The Agent improves its policy"),xt=u(),G=i("h3"),pe=i("a"),jl=n("Exploitation and Exploration"),bt=u(),F=i("h3"),Ee=i("a"),Zl=n("Prediction and Control:"),It=u(),k=i("ul"),Qe=i("li"),Wl=n("Prediction: Evaluate the future, given a policy"),Xl=u(),et=i("li"),Yl=n("Control: Optimize the future and find the best policy"),this.h()},l(e){c=r(e,"H1",{id:!0});var o=a(c);h=r(o,"A",{href:!0});var si=a(h);x=s(si,"Reinforcement Learning Course Notes: David Silver’s Course"),si.forEach(t),o.forEach(t),L=d(e),m=r(e,"H1",{id:!0});var fi=a(m);_=r(fi,"A",{href:!0});var ui=a(_);j=s(ui,"Lecture 1: Introduction to RL"),ui.forEach(t),fi.forEach(t),tt=d(e),Z=r(e,"P",{});var di=a(Z);Pt=s(di,"Created: May 6, 2023 9:27 PM"),di.forEach(t),lt=d(e),P=r(e,"H3",{id:!0});var ci=a(P);W=r(ci,"A",{href:!0});var vi=a(W);zt=s(vi,"Branches of Machine learning:"),vi.forEach(t),ci.forEach(t),it=d(e),g=r(e,"UL",{});var Le=a(g);be=r(Le,"LI",{});var mi=a(be);Ht=s(mi,"Supervised Learning"),mi.forEach(t),Mt=d(Le),Ie=r(Le,"LI",{});var hi=a(Ie);qt=s(hi,"Unsupervised Learning"),hi.forEach(t),Ut=d(Le),Re=r(Le,"LI",{});var _i=a(Re);Nt=s(_i,"Reinforcement Learning"),_i.forEach(t),Le.forEach(t),rt=d(e),z=r(e,"H3",{id:!0});var pi=a(z);X=r(pi,"A",{href:!0});var Ei=a(X);Bt=s(Ei,"Rewards:"),Ei.forEach(t),pi.forEach(t),at=d(e),R=r(e,"UL",{});var Rt=a(R);Se=r(Rt,"LI",{});var Li=a(Se);Dt=s(Li,"A reward is a scalar feedback signal"),Li.forEach(t),$t=d(Rt),Te=r(Rt,"LI",{});var gi=a(Te);Ot=s(gi,"Agent’s job is to maximize cumulative reward"),gi.forEach(t),Rt.forEach(t),ot=d(e),H=r(e,"H3",{id:!0});var wi=a(H);Y=r(wi,"A",{href:!0});var yi=a(Y);Vt=s(yi,"Reward Hypothesis:"),yi.forEach(t),wi.forEach(t),nt=d(e),J=r(e,"UL",{});var Ai=a(J);ke=r(Ai,"LI",{});var xi=a(ke);Gt=s(xi,"All goals cam be described by the maximization of expected cumulative reward"),xi.forEach(t),Ai.forEach(t),st=d(e),M=r(e,"H3",{id:!0});var bi=a(M);K=r(bi,"A",{href:!0});var Ii=a(K);Ft=s(Ii,"Sequential Decision making:"),Ii.forEach(t),bi.forEach(t),ft=d(e),E=r(e,"UL",{});var C=a(E);Q=r(C,"LI",{});var Jl=a(Q);Ce=r(Jl,"STRONG",{});var Ri=a(Ce);jt=s(Ri,"Goal:"),Ri.forEach(t),Zt=s(Jl,"  Select actions to maximize total future reward"),Jl.forEach(t),Wt=d(C),Pe=r(C,"LI",{});var Si=a(Pe);Xt=s(Si,"Actions may have long term consequences"),Si.forEach(t),Yt=d(C),ze=r(C,"LI",{});var Ti=a(ze);Jt=s(Ti,"Reward may be delayed"),Ti.forEach(t),Kt=d(C),He=r(C,"LI",{});var ki=a(He);Qt=s(ki,"It may be better to sacrifice immediate reward to gain more long-term reward"),ki.forEach(t),C.forEach(t),ut=d(e),q=r(e,"H3",{id:!0});var Ci=a(q);ee=r(Ci,"A",{href:!0});var Pi=a(ee);el=s(Pi,"Agent and Environment:"),Pi.forEach(t),Ci.forEach(t),dt=d(e),te=r(e,"UL",{});var zi=a(te);le=r(zi,"LI",{});var Kl=a(le);tl=s(Kl,"At Each step t, the agent:"),U=r(Kl,"UL",{});var St=a(U);ie=r(St,"LI",{});var Ql=a(ie);ll=s(Ql,"Executes an action "),ct=Pr(Ql,!1),Ql.forEach(t),il=d(St),Me=r(St,"LI",{});var gr=a(Me);gr.forEach(t),St.forEach(t),Kl.forEach(t),zi.forEach(t),vt=d(e),N=r(e,"H3",{id:!0});var Hi=a(N);re=r(Hi,"A",{href:!0});var Mi=a(re);rl=s(Mi,"Value Function:"),Mi.forEach(t),Hi.forEach(t),mt=d(e),w=r(e,"UL",{});var ge=a(w);qe=r(ge,"LI",{});var qi=a(qe);al=s(qi,"Prediction of future reward"),qi.forEach(t),ol=d(ge),Ue=r(ge,"LI",{});var Ui=a(Ue);nl=s(Ui,"used to evaluate the goodness/badness of state"),Ui.forEach(t),sl=d(ge),Ne=r(ge,"LI",{});var Ni=a(Ne);fl=s(Ni,"select between actions"),Ni.forEach(t),ge.forEach(t),ht=d(e),B=r(e,"H3",{id:!0});var Bi=a(B);ae=r(Bi,"A",{href:!0});var Di=a(ae);ul=s(Di,"Model:"),Di.forEach(t),Bi.forEach(t),_t=d(e),y=r(e,"UL",{});var we=a(y);Be=r(we,"LI",{});var $i=a(Be);dl=s($i,"A model predicts what environment will do next"),$i.forEach(t),cl=d(we),De=r(we,"LI",{});var Oi=a(De);vl=s(Oi,"P predicts the next state"),Oi.forEach(t),ml=d(we),$e=r(we,"LI",{});var Vi=a($e);hl=s(Vi,"R predicts the reward"),Vi.forEach(t),we.forEach(t),pt=d(e),D=r(e,"H3",{id:!0});var Gi=a(D);oe=r(Gi,"A",{href:!0});var Fi=a(oe);_l=s(Fi,"Categorizing RL Agents (1):"),Fi.forEach(t),Gi.forEach(t),Et=d(e),A=r(e,"UL",{});var ye=a(A);ne=r(ye,"LI",{});var ei=a(ne);Oe=r(ei,"STRONG",{});var ji=a(Oe);pl=s(ji,"Value Function:"),ji.forEach(t),El=s(ei,"  No policy but value function"),ei.forEach(t),Ll=d(ye),se=r(ye,"LI",{});var ti=a(se);Ve=r(ti,"STRONG",{});var Zi=a(Ve);gl=s(Zi,"Policy Based:"),Zi.forEach(t),wl=s(ti,"  No value function but policy"),ti.forEach(t),yl=d(ye),fe=r(ye,"LI",{});var li=a(fe);Ge=r(li,"STRONG",{});var Wi=a(Ge);Al=s(Wi,"Actor-Critic:"),Wi.forEach(t),xl=s(li," Both policy and value function"),li.forEach(t),ye.forEach(t),Lt=d(e),$=r(e,"H3",{id:!0});var Xi=a($);ue=r(Xi,"A",{href:!0});var Yi=a(ue);bl=s(Yi,"Categorizing RL Agents (2):"),Yi.forEach(t),Xi.forEach(t),gt=d(e),S=r(e,"UL",{});var Tt=a(S);de=r(Tt,"LI",{});var ii=a(de);Fe=r(ii,"STRONG",{});var Ji=a(Fe);Il=s(Ji,"Model-Free:"),Ji.forEach(t),Rl=s(ii," Policy and/or Value function"),ii.forEach(t),Sl=d(Tt),ce=r(Tt,"LI",{});var ri=a(ce);je=r(ri,"STRONG",{});var Ki=a(je);Tl=s(Ki,"Model-Based:"),Ki.forEach(t),kl=s(ri," Policy and/or Value function and Model"),ri.forEach(t),Tt.forEach(t),wt=d(e),O=r(e,"H3",{id:!0});var Qi=a(O);ve=r(Qi,"A",{href:!0});var er=a(ve);Cl=s(er,"RL Agent Taxonomy:"),er.forEach(t),Qi.forEach(t),yt=d(e),V=r(e,"H3",{id:!0});var tr=a(V);me=r(tr,"A",{href:!0});var lr=a(me);Pl=s(lr,"Two fundamental problems in Sequential Decision making:"),lr.forEach(t),tr.forEach(t),At=d(e),T=r(e,"UL",{});var kt=a(T);he=r(kt,"LI",{});var ai=a(he);zl=s(ai,"RL :"),b=r(ai,"UL",{});var Ae=a(b);Ze=r(Ae,"LI",{});var ir=a(Ze);Hl=s(ir,"The environment is initially unknown"),ir.forEach(t),Ml=d(Ae),We=r(Ae,"LI",{});var rr=a(We);ql=s(rr,"The Agent interacts with environment"),rr.forEach(t),Ul=d(Ae),Xe=r(Ae,"LI",{});var ar=a(Xe);Nl=s(ar,"The Agent improves its policy"),ar.forEach(t),Ae.forEach(t),ai.forEach(t),Bl=d(kt),_e=r(kt,"LI",{});var oi=a(_e);Dl=s(oi,"Planning:"),I=r(oi,"UL",{});var xe=a(I);Ye=r(xe,"LI",{});var or=a(Ye);$l=s(or,"A model of env. is known"),or.forEach(t),Ol=d(xe),Je=r(xe,"LI",{});var nr=a(Je);Vl=s(nr,"Agent performs computation with model"),nr.forEach(t),Gl=d(xe),Ke=r(xe,"LI",{});var sr=a(Ke);Fl=s(sr,"The Agent improves its policy"),sr.forEach(t),xe.forEach(t),oi.forEach(t),kt.forEach(t),xt=d(e),G=r(e,"H3",{id:!0});var fr=a(G);pe=r(fr,"A",{href:!0});var ur=a(pe);jl=s(ur,"Exploitation and Exploration"),ur.forEach(t),fr.forEach(t),bt=d(e),F=r(e,"H3",{id:!0});var dr=a(F);Ee=r(dr,"A",{href:!0});var cr=a(Ee);Zl=s(cr,"Prediction and Control:"),cr.forEach(t),dr.forEach(t),It=d(e),k=r(e,"UL",{});var Ct=a(k);Qe=r(Ct,"LI",{});var vr=a(Qe);Wl=s(vr,"Prediction: Evaluate the future, given a policy"),vr.forEach(t),Xl=d(Ct),et=r(Ct,"LI",{});var mr=a(et);Yl=s(mr,"Control: Optimize the future and find the best policy"),mr.forEach(t),Ct.forEach(t),this.h()},h(){v(h,"href","#reinforcement-learning-course-notes-david-silvers-course"),v(c,"id","reinforcement-learning-course-notes-david-silvers-course"),v(_,"href","#lecture-1-introduction-to-rl"),v(m,"id","lecture-1-introduction-to-rl"),v(W,"href","#branches-of-machine-learning"),v(P,"id","branches-of-machine-learning"),v(X,"href","#rewards"),v(z,"id","rewards"),v(Y,"href","#reward-hypothesis"),v(H,"id","reward-hypothesis"),v(K,"href","#sequential-decision-making"),v(M,"id","sequential-decision-making"),v(ee,"href","#agent-and-environment"),v(q,"id","agent-and-environment"),ct.a=null,v(re,"href","#value-function"),v(N,"id","value-function"),v(ae,"href","#model"),v(B,"id","model"),v(oe,"href","#categorizing-rl-agents-1"),v(D,"id","categorizing-rl-agents-1"),v(ue,"href","#categorizing-rl-agents-2"),v($,"id","categorizing-rl-agents-2"),v(ve,"href","#rl-agent-taxonomy"),v(O,"id","rl-agent-taxonomy"),v(me,"href","#two-fundamental-problems-in-sequential-decision-making"),v(V,"id","two-fundamental-problems-in-sequential-decision-making"),v(pe,"href","#exploitation-and-exploration"),v(G,"id","exploitation-and-exploration"),v(Ee,"href","#prediction-and-control"),v(F,"id","prediction-and-control")},m(e,o){f(e,c,o),l(c,h),l(h,x),f(e,L,o),f(e,m,o),l(m,_),l(_,j),f(e,tt,o),f(e,Z,o),l(Z,Pt),f(e,lt,o),f(e,P,o),l(P,W),l(W,zt),f(e,it,o),f(e,g,o),l(g,be),l(be,Ht),l(g,Mt),l(g,Ie),l(Ie,qt),l(g,Ut),l(g,Re),l(Re,Nt),f(e,rt,o),f(e,z,o),l(z,X),l(X,Bt),f(e,at,o),f(e,R,o),l(R,Se),l(Se,Dt),l(R,$t),l(R,Te),l(Te,Ot),f(e,ot,o),f(e,H,o),l(H,Y),l(Y,Vt),f(e,nt,o),f(e,J,o),l(J,ke),l(ke,Gt),f(e,st,o),f(e,M,o),l(M,K),l(K,Ft),f(e,ft,o),f(e,E,o),l(E,Q),l(Q,Ce),l(Ce,jt),l(Q,Zt),l(E,Wt),l(E,Pe),l(Pe,Xt),l(E,Yt),l(E,ze),l(ze,Jt),l(E,Kt),l(E,He),l(He,Qt),f(e,ut,o),f(e,q,o),l(q,ee),l(ee,el),f(e,dt,o),f(e,te,o),l(te,le),l(le,tl),l(le,U),l(U,ie),l(ie,ll),ct.m(p[0],ie),l(U,il),l(U,Me),Me.innerHTML=p[1],f(e,vt,o),f(e,N,o),l(N,re),l(re,rl),f(e,mt,o),f(e,w,o),l(w,qe),l(qe,al),l(w,ol),l(w,Ue),l(Ue,nl),l(w,sl),l(w,Ne),l(Ne,fl),f(e,ht,o),f(e,B,o),l(B,ae),l(ae,ul),f(e,_t,o),f(e,y,o),l(y,Be),l(Be,dl),l(y,cl),l(y,De),l(De,vl),l(y,ml),l(y,$e),l($e,hl),f(e,pt,o),f(e,D,o),l(D,oe),l(oe,_l),f(e,Et,o),f(e,A,o),l(A,ne),l(ne,Oe),l(Oe,pl),l(ne,El),l(A,Ll),l(A,se),l(se,Ve),l(Ve,gl),l(se,wl),l(A,yl),l(A,fe),l(fe,Ge),l(Ge,Al),l(fe,xl),f(e,Lt,o),f(e,$,o),l($,ue),l(ue,bl),f(e,gt,o),f(e,S,o),l(S,de),l(de,Fe),l(Fe,Il),l(de,Rl),l(S,Sl),l(S,ce),l(ce,je),l(je,Tl),l(ce,kl),f(e,wt,o),f(e,O,o),l(O,ve),l(ve,Cl),f(e,yt,o),f(e,V,o),l(V,me),l(me,Pl),f(e,At,o),f(e,T,o),l(T,he),l(he,zl),l(he,b),l(b,Ze),l(Ze,Hl),l(b,Ml),l(b,We),l(We,ql),l(b,Ul),l(b,Xe),l(Xe,Nl),l(T,Bl),l(T,_e),l(_e,Dl),l(_e,I),l(I,Ye),l(Ye,$l),l(I,Ol),l(I,Je),l(Je,Vl),l(I,Gl),l(I,Ke),l(Ke,Fl),f(e,xt,o),f(e,G,o),l(G,pe),l(pe,jl),f(e,bt,o),f(e,F,o),l(F,Ee),l(Ee,Zl),f(e,It,o),f(e,k,o),l(k,Qe),l(Qe,Wl),l(k,Xl),l(k,et),l(et,Yl)},p:zr,d(e){e&&t(c),e&&t(L),e&&t(m),e&&t(tt),e&&t(Z),e&&t(lt),e&&t(P),e&&t(it),e&&t(g),e&&t(rt),e&&t(z),e&&t(at),e&&t(R),e&&t(ot),e&&t(H),e&&t(nt),e&&t(J),e&&t(st),e&&t(M),e&&t(ft),e&&t(E),e&&t(ut),e&&t(q),e&&t(dt),e&&t(te),e&&t(vt),e&&t(N),e&&t(mt),e&&t(w),e&&t(ht),e&&t(B),e&&t(_t),e&&t(y),e&&t(pt),e&&t(D),e&&t(Et),e&&t(A),e&&t(Lt),e&&t($),e&&t(gt),e&&t(S),e&&t(wt),e&&t(O),e&&t(yt),e&&t(V),e&&t(At),e&&t(T),e&&t(xt),e&&t(G),e&&t(bt),e&&t(F),e&&t(It),e&&t(k)}}}function Ur(p){let c,h;const x=[p[2],Er];let L={$$slots:{default:[qr]},$$scope:{ctx:p}};for(let m=0;m<x.length;m+=1)L=ni(L,x[m]);return c=new Hr({props:L}),{c(){xr(c.$$.fragment)},l(m){br(c.$$.fragment,m)},m(m,_){Ir(c,m,_),h=!0},p(m,[_]){const j=_&4?Rr(x,[_&4&&hr(m[2]),_&0&&hr(Er)]):{};_&16&&(j.$$scope={dirty:_,ctx:m}),c.$set(j)},i(m){h||(Sr(c.$$.fragment,m),h=!0)},o(m){Tr(c.$$.fragment,m),h=!1},d(m){kr(c,m)}}}const Er={title:"Reinforcement Learning Lecture Notes",summary:"Lectures notes from David Silver's Reinforcement Learning Course",created:"2023-05-21T00:00:00.000Z",tags:["Reinforcement Learning"],toc:[{depth:1,title:"Reinforcement Learning Course Notes: David Silver’s Course",slug:"reinforcement-learning-course-notes-david-silvers-course"},{depth:1,title:"Lecture 1: Introduction to RL",slug:"lecture-1-introduction-to-rl"},{depth:3,title:"Branches of Machine learning:",slug:"branches-of-machine-learning"},{depth:3,title:"Rewards:",slug:"rewards"},{depth:3,title:"Reward Hypothesis:",slug:"reward-hypothesis"},{depth:3,title:"Sequential Decision making:",slug:"sequential-decision-making"},{depth:3,title:"Agent and Environment:",slug:"agent-and-environment"},{depth:3,title:"Value Function:",slug:"value-function"},{depth:3,title:"Model:",slug:"model"},{depth:3,title:"Categorizing RL Agents (1):",slug:"categorizing-rl-agents-1"},{depth:3,title:"Categorizing RL Agents (2):",slug:"categorizing-rl-agents-2"},{depth:3,title:"RL Agent Taxonomy:",slug:"rl-agent-taxonomy"},{depth:3,title:"Two fundamental problems in Sequential Decision making:",slug:"two-fundamental-problems-in-sequential-decision-making"},{depth:3,title:"Exploitation and Exploration",slug:"exploitation-and-exploration"},{depth:3,title:"Prediction and Control:",slug:"prediction-and-control"}],flags:[],updated:"2023-06-10T14:49:27.187Z",slug:"/reinforcement-learning/+page.svelte.md",path:"/reinforcement-learning"};function Nr(p,c,h){const x=pr("ax^2+bx+c=0");Mr("x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}");const L=pr("e^{i\\pi} = -1",{wrap:!0});return p.$$set=m=>{h(2,c=ni(ni({},c),_r(m)))},c=_r(c),[x,L,c]}class Or extends wr{constructor(c){super(),yr(this,c,Nr,Ur,Ar,{})}}export{Or as default,Er as metadata};
