import{S as Qi,i as Yi,s as eo,U as Ol,y as to,z as lo,A as io,V as oo,W as Xi,g as ro,d as ao,B as no,X as Ji,k as i,q as n,a as u,l as o,m as r,r as s,h as t,c as d,n as c,b as f,D as l,E as so}from"../chunks/index.34fbf63c.js";import{P as fo}from"../chunks/post_layout.8aa90ea2.js";function uo(G){let m,_,w,R,v,h,O,We,F,yt,Xe,T,j,wt,Je,E,Le,At,xt,ge,Rt,bt,ye,It,Ke,z,Z,Ct,Qe,b,we,Pt,St,Ae,Tt,Ye,k,W,zt,et,X,xe,kt,tt,H,J,Ht,lt,p,K,Re,qt,Mt,Ut,be,Nt,Dt,Ie,Bt,Vt,Ce,$t,it,q,Q,Gt,ot,L,Pe,Ot,Ft,Se,jt,Zt,Te,Wt,rt,M,Y,Xt,at,g,ze,Jt,Kt,ke,Qt,Yt,He,el,nt,U,ee,tl,st,y,te,qe,ll,il,ol,le,Me,rl,al,nl,ie,Ue,sl,fl,ft,N,oe,ul,ut,I,re,Ne,dl,cl,ml,ae,De,vl,hl,dt,D,ne,_l,ct,B,se,pl,mt,C,fe,El,A,Be,Ll,gl,Ve,yl,wl,$e,Al,xl,ue,Rl,x,Ge,bl,Il,Oe,Cl,Pl,Fe,Sl,vt,V,de,Tl,ht,$,ce,zl,_t,P,je,kl,Hl,Ze,ql;return{c(){m=i("h1"),_=i("a"),w=n("Reinforcement Learning Course Notes: David Silver’s Course"),R=u(),v=i("h1"),h=i("a"),O=n("Lecture 1: Introduction to RL"),We=u(),F=i("p"),yt=n("Created: May 6, 2023 9:27 PM"),Xe=u(),T=i("h3"),j=i("a"),wt=n("Branches of Machine learning:"),Je=u(),E=i("ul"),Le=i("li"),At=n("Supervised Learning"),xt=u(),ge=i("li"),Rt=n("Unsupervised Learning"),bt=u(),ye=i("li"),It=n("Reinforcement Learning"),Ke=u(),z=i("h3"),Z=i("a"),Ct=n("Rewards:"),Qe=u(),b=i("ul"),we=i("li"),Pt=n("A reward is a scalar feedback signal"),St=u(),Ae=i("li"),Tt=n("Agent’s job is to maximize cumulative reward"),Ye=u(),k=i("h3"),W=i("a"),zt=n("Reward Hypothesis:"),et=u(),X=i("ul"),xe=i("li"),kt=n("All goals cam be described by the maximization of expected cumulative reward"),tt=u(),H=i("h3"),J=i("a"),Ht=n("Sequential Decision making:"),lt=u(),p=i("ul"),K=i("li"),Re=i("strong"),qt=n("Goal:"),Mt=n("  Select actions to maximize total future reward"),Ut=u(),be=i("li"),Nt=n("Actions may have long term consequences"),Dt=u(),Ie=i("li"),Bt=n("Reward may be delayed"),Vt=u(),Ce=i("li"),$t=n("It may be better to sacrifice immediate reward to gain more long-term reward"),it=u(),q=i("h3"),Q=i("a"),Gt=n("Value Function:"),ot=u(),L=i("ul"),Pe=i("li"),Ot=n("Prediction of future reward"),Ft=u(),Se=i("li"),jt=n("used to evaluate the goodness/badness of state"),Zt=u(),Te=i("li"),Wt=n("select between actions"),rt=u(),M=i("h3"),Y=i("a"),Xt=n("Model:"),at=u(),g=i("ul"),ze=i("li"),Jt=n("A model predicts what environment will do next"),Kt=u(),ke=i("li"),Qt=n("P predicts the next state"),Yt=u(),He=i("li"),el=n("R predicts the reward"),nt=u(),U=i("h3"),ee=i("a"),tl=n("Categorizing RL Agents (1):"),st=u(),y=i("ul"),te=i("li"),qe=i("strong"),ll=n("Value Function:"),il=n("  No policy but value function"),ol=u(),le=i("li"),Me=i("strong"),rl=n("Policy Based:"),al=n("  No value function but policy"),nl=u(),ie=i("li"),Ue=i("strong"),sl=n("Actor-Critic:"),fl=n(" Both policy and value function"),ft=u(),N=i("h3"),oe=i("a"),ul=n("Categorizing RL Agents (2):"),ut=u(),I=i("ul"),re=i("li"),Ne=i("strong"),dl=n("Model-Free:"),cl=n(" Policy and/or Value function"),ml=u(),ae=i("li"),De=i("strong"),vl=n("Model-Based:"),hl=n(" Policy and/or Value function and Model"),dt=u(),D=i("h3"),ne=i("a"),_l=n("RL Agent Taxonomy:"),ct=u(),B=i("h3"),se=i("a"),pl=n("Two fundamental problems in Sequential Decision making:"),mt=u(),C=i("ul"),fe=i("li"),El=n("RL :"),A=i("ul"),Be=i("li"),Ll=n("The environment is initially unknown"),gl=u(),Ve=i("li"),yl=n("The Agent interacts with environment"),wl=u(),$e=i("li"),Al=n("The Agent improves its policy"),xl=u(),ue=i("li"),Rl=n("Planning:"),x=i("ul"),Ge=i("li"),bl=n("A model of env. is known"),Il=u(),Oe=i("li"),Cl=n("Agent performs computation with model"),Pl=u(),Fe=i("li"),Sl=n("The Agent improves its policy"),vt=u(),V=i("h3"),de=i("a"),Tl=n("Exploitation and Exploration"),ht=u(),$=i("h3"),ce=i("a"),zl=n("Prediction and Control:"),_t=u(),P=i("ul"),je=i("li"),kl=n("Prediction: Evaluate the future, given a policy"),Hl=u(),Ze=i("li"),ql=n("Control: Optimize the future and find the best policy"),this.h()},l(e){m=o(e,"H1",{id:!0});var a=r(m);_=o(a,"A",{href:!0});var Fl=r(_);w=s(Fl,"Reinforcement Learning Course Notes: David Silver’s Course"),Fl.forEach(t),a.forEach(t),R=d(e),v=o(e,"H1",{id:!0});var jl=r(v);h=o(jl,"A",{href:!0});var Zl=r(h);O=s(Zl,"Lecture 1: Introduction to RL"),Zl.forEach(t),jl.forEach(t),We=d(e),F=o(e,"P",{});var Wl=r(F);yt=s(Wl,"Created: May 6, 2023 9:27 PM"),Wl.forEach(t),Xe=d(e),T=o(e,"H3",{id:!0});var Xl=r(T);j=o(Xl,"A",{href:!0});var Jl=r(j);wt=s(Jl,"Branches of Machine learning:"),Jl.forEach(t),Xl.forEach(t),Je=d(e),E=o(e,"UL",{});var me=r(E);Le=o(me,"LI",{});var Kl=r(Le);At=s(Kl,"Supervised Learning"),Kl.forEach(t),xt=d(me),ge=o(me,"LI",{});var Ql=r(ge);Rt=s(Ql,"Unsupervised Learning"),Ql.forEach(t),bt=d(me),ye=o(me,"LI",{});var Yl=r(ye);It=s(Yl,"Reinforcement Learning"),Yl.forEach(t),me.forEach(t),Ke=d(e),z=o(e,"H3",{id:!0});var ei=r(z);Z=o(ei,"A",{href:!0});var ti=r(Z);Ct=s(ti,"Rewards:"),ti.forEach(t),ei.forEach(t),Qe=d(e),b=o(e,"UL",{});var pt=r(b);we=o(pt,"LI",{});var li=r(we);Pt=s(li,"A reward is a scalar feedback signal"),li.forEach(t),St=d(pt),Ae=o(pt,"LI",{});var ii=r(Ae);Tt=s(ii,"Agent’s job is to maximize cumulative reward"),ii.forEach(t),pt.forEach(t),Ye=d(e),k=o(e,"H3",{id:!0});var oi=r(k);W=o(oi,"A",{href:!0});var ri=r(W);zt=s(ri,"Reward Hypothesis:"),ri.forEach(t),oi.forEach(t),et=d(e),X=o(e,"UL",{});var ai=r(X);xe=o(ai,"LI",{});var ni=r(xe);kt=s(ni,"All goals cam be described by the maximization of expected cumulative reward"),ni.forEach(t),ai.forEach(t),tt=d(e),H=o(e,"H3",{id:!0});var si=r(H);J=o(si,"A",{href:!0});var fi=r(J);Ht=s(fi,"Sequential Decision making:"),fi.forEach(t),si.forEach(t),lt=d(e),p=o(e,"UL",{});var S=r(p);K=o(S,"LI",{});var Ml=r(K);Re=o(Ml,"STRONG",{});var ui=r(Re);qt=s(ui,"Goal:"),ui.forEach(t),Mt=s(Ml,"  Select actions to maximize total future reward"),Ml.forEach(t),Ut=d(S),be=o(S,"LI",{});var di=r(be);Nt=s(di,"Actions may have long term consequences"),di.forEach(t),Dt=d(S),Ie=o(S,"LI",{});var ci=r(Ie);Bt=s(ci,"Reward may be delayed"),ci.forEach(t),Vt=d(S),Ce=o(S,"LI",{});var mi=r(Ce);$t=s(mi,"It may be better to sacrifice immediate reward to gain more long-term reward"),mi.forEach(t),S.forEach(t),it=d(e),q=o(e,"H3",{id:!0});var vi=r(q);Q=o(vi,"A",{href:!0});var hi=r(Q);Gt=s(hi,"Value Function:"),hi.forEach(t),vi.forEach(t),ot=d(e),L=o(e,"UL",{});var ve=r(L);Pe=o(ve,"LI",{});var _i=r(Pe);Ot=s(_i,"Prediction of future reward"),_i.forEach(t),Ft=d(ve),Se=o(ve,"LI",{});var pi=r(Se);jt=s(pi,"used to evaluate the goodness/badness of state"),pi.forEach(t),Zt=d(ve),Te=o(ve,"LI",{});var Ei=r(Te);Wt=s(Ei,"select between actions"),Ei.forEach(t),ve.forEach(t),rt=d(e),M=o(e,"H3",{id:!0});var Li=r(M);Y=o(Li,"A",{href:!0});var gi=r(Y);Xt=s(gi,"Model:"),gi.forEach(t),Li.forEach(t),at=d(e),g=o(e,"UL",{});var he=r(g);ze=o(he,"LI",{});var yi=r(ze);Jt=s(yi,"A model predicts what environment will do next"),yi.forEach(t),Kt=d(he),ke=o(he,"LI",{});var wi=r(ke);Qt=s(wi,"P predicts the next state"),wi.forEach(t),Yt=d(he),He=o(he,"LI",{});var Ai=r(He);el=s(Ai,"R predicts the reward"),Ai.forEach(t),he.forEach(t),nt=d(e),U=o(e,"H3",{id:!0});var xi=r(U);ee=o(xi,"A",{href:!0});var Ri=r(ee);tl=s(Ri,"Categorizing RL Agents (1):"),Ri.forEach(t),xi.forEach(t),st=d(e),y=o(e,"UL",{});var _e=r(y);te=o(_e,"LI",{});var Ul=r(te);qe=o(Ul,"STRONG",{});var bi=r(qe);ll=s(bi,"Value Function:"),bi.forEach(t),il=s(Ul,"  No policy but value function"),Ul.forEach(t),ol=d(_e),le=o(_e,"LI",{});var Nl=r(le);Me=o(Nl,"STRONG",{});var Ii=r(Me);rl=s(Ii,"Policy Based:"),Ii.forEach(t),al=s(Nl,"  No value function but policy"),Nl.forEach(t),nl=d(_e),ie=o(_e,"LI",{});var Dl=r(ie);Ue=o(Dl,"STRONG",{});var Ci=r(Ue);sl=s(Ci,"Actor-Critic:"),Ci.forEach(t),fl=s(Dl," Both policy and value function"),Dl.forEach(t),_e.forEach(t),ft=d(e),N=o(e,"H3",{id:!0});var Pi=r(N);oe=o(Pi,"A",{href:!0});var Si=r(oe);ul=s(Si,"Categorizing RL Agents (2):"),Si.forEach(t),Pi.forEach(t),ut=d(e),I=o(e,"UL",{});var Et=r(I);re=o(Et,"LI",{});var Bl=r(re);Ne=o(Bl,"STRONG",{});var Ti=r(Ne);dl=s(Ti,"Model-Free:"),Ti.forEach(t),cl=s(Bl," Policy and/or Value function"),Bl.forEach(t),ml=d(Et),ae=o(Et,"LI",{});var Vl=r(ae);De=o(Vl,"STRONG",{});var zi=r(De);vl=s(zi,"Model-Based:"),zi.forEach(t),hl=s(Vl," Policy and/or Value function and Model"),Vl.forEach(t),Et.forEach(t),dt=d(e),D=o(e,"H3",{id:!0});var ki=r(D);ne=o(ki,"A",{href:!0});var Hi=r(ne);_l=s(Hi,"RL Agent Taxonomy:"),Hi.forEach(t),ki.forEach(t),ct=d(e),B=o(e,"H3",{id:!0});var qi=r(B);se=o(qi,"A",{href:!0});var Mi=r(se);pl=s(Mi,"Two fundamental problems in Sequential Decision making:"),Mi.forEach(t),qi.forEach(t),mt=d(e),C=o(e,"UL",{});var Lt=r(C);fe=o(Lt,"LI",{});var $l=r(fe);El=s($l,"RL :"),A=o($l,"UL",{});var pe=r(A);Be=o(pe,"LI",{});var Ui=r(Be);Ll=s(Ui,"The environment is initially unknown"),Ui.forEach(t),gl=d(pe),Ve=o(pe,"LI",{});var Ni=r(Ve);yl=s(Ni,"The Agent interacts with environment"),Ni.forEach(t),wl=d(pe),$e=o(pe,"LI",{});var Di=r($e);Al=s(Di,"The Agent improves its policy"),Di.forEach(t),pe.forEach(t),$l.forEach(t),xl=d(Lt),ue=o(Lt,"LI",{});var Gl=r(ue);Rl=s(Gl,"Planning:"),x=o(Gl,"UL",{});var Ee=r(x);Ge=o(Ee,"LI",{});var Bi=r(Ge);bl=s(Bi,"A model of env. is known"),Bi.forEach(t),Il=d(Ee),Oe=o(Ee,"LI",{});var Vi=r(Oe);Cl=s(Vi,"Agent performs computation with model"),Vi.forEach(t),Pl=d(Ee),Fe=o(Ee,"LI",{});var $i=r(Fe);Sl=s($i,"The Agent improves its policy"),$i.forEach(t),Ee.forEach(t),Gl.forEach(t),Lt.forEach(t),vt=d(e),V=o(e,"H3",{id:!0});var Gi=r(V);de=o(Gi,"A",{href:!0});var Oi=r(de);Tl=s(Oi,"Exploitation and Exploration"),Oi.forEach(t),Gi.forEach(t),ht=d(e),$=o(e,"H3",{id:!0});var Fi=r($);ce=o(Fi,"A",{href:!0});var ji=r(ce);zl=s(ji,"Prediction and Control:"),ji.forEach(t),Fi.forEach(t),_t=d(e),P=o(e,"UL",{});var gt=r(P);je=o(gt,"LI",{});var Zi=r(je);kl=s(Zi,"Prediction: Evaluate the future, given a policy"),Zi.forEach(t),Hl=d(gt),Ze=o(gt,"LI",{});var Wi=r(Ze);ql=s(Wi,"Control: Optimize the future and find the best policy"),Wi.forEach(t),gt.forEach(t),this.h()},h(){c(_,"href","#reinforcement-learning-course-notes-david-silvers-course"),c(m,"id","reinforcement-learning-course-notes-david-silvers-course"),c(h,"href","#lecture-1-introduction-to-rl"),c(v,"id","lecture-1-introduction-to-rl"),c(j,"href","#branches-of-machine-learning"),c(T,"id","branches-of-machine-learning"),c(Z,"href","#rewards"),c(z,"id","rewards"),c(W,"href","#reward-hypothesis"),c(k,"id","reward-hypothesis"),c(J,"href","#sequential-decision-making"),c(H,"id","sequential-decision-making"),c(Q,"href","#value-function"),c(q,"id","value-function"),c(Y,"href","#model"),c(M,"id","model"),c(ee,"href","#categorizing-rl-agents-1"),c(U,"id","categorizing-rl-agents-1"),c(oe,"href","#categorizing-rl-agents-2"),c(N,"id","categorizing-rl-agents-2"),c(ne,"href","#rl-agent-taxonomy"),c(D,"id","rl-agent-taxonomy"),c(se,"href","#two-fundamental-problems-in-sequential-decision-making"),c(B,"id","two-fundamental-problems-in-sequential-decision-making"),c(de,"href","#exploitation-and-exploration"),c(V,"id","exploitation-and-exploration"),c(ce,"href","#prediction-and-control"),c($,"id","prediction-and-control")},m(e,a){f(e,m,a),l(m,_),l(_,w),f(e,R,a),f(e,v,a),l(v,h),l(h,O),f(e,We,a),f(e,F,a),l(F,yt),f(e,Xe,a),f(e,T,a),l(T,j),l(j,wt),f(e,Je,a),f(e,E,a),l(E,Le),l(Le,At),l(E,xt),l(E,ge),l(ge,Rt),l(E,bt),l(E,ye),l(ye,It),f(e,Ke,a),f(e,z,a),l(z,Z),l(Z,Ct),f(e,Qe,a),f(e,b,a),l(b,we),l(we,Pt),l(b,St),l(b,Ae),l(Ae,Tt),f(e,Ye,a),f(e,k,a),l(k,W),l(W,zt),f(e,et,a),f(e,X,a),l(X,xe),l(xe,kt),f(e,tt,a),f(e,H,a),l(H,J),l(J,Ht),f(e,lt,a),f(e,p,a),l(p,K),l(K,Re),l(Re,qt),l(K,Mt),l(p,Ut),l(p,be),l(be,Nt),l(p,Dt),l(p,Ie),l(Ie,Bt),l(p,Vt),l(p,Ce),l(Ce,$t),f(e,it,a),f(e,q,a),l(q,Q),l(Q,Gt),f(e,ot,a),f(e,L,a),l(L,Pe),l(Pe,Ot),l(L,Ft),l(L,Se),l(Se,jt),l(L,Zt),l(L,Te),l(Te,Wt),f(e,rt,a),f(e,M,a),l(M,Y),l(Y,Xt),f(e,at,a),f(e,g,a),l(g,ze),l(ze,Jt),l(g,Kt),l(g,ke),l(ke,Qt),l(g,Yt),l(g,He),l(He,el),f(e,nt,a),f(e,U,a),l(U,ee),l(ee,tl),f(e,st,a),f(e,y,a),l(y,te),l(te,qe),l(qe,ll),l(te,il),l(y,ol),l(y,le),l(le,Me),l(Me,rl),l(le,al),l(y,nl),l(y,ie),l(ie,Ue),l(Ue,sl),l(ie,fl),f(e,ft,a),f(e,N,a),l(N,oe),l(oe,ul),f(e,ut,a),f(e,I,a),l(I,re),l(re,Ne),l(Ne,dl),l(re,cl),l(I,ml),l(I,ae),l(ae,De),l(De,vl),l(ae,hl),f(e,dt,a),f(e,D,a),l(D,ne),l(ne,_l),f(e,ct,a),f(e,B,a),l(B,se),l(se,pl),f(e,mt,a),f(e,C,a),l(C,fe),l(fe,El),l(fe,A),l(A,Be),l(Be,Ll),l(A,gl),l(A,Ve),l(Ve,yl),l(A,wl),l(A,$e),l($e,Al),l(C,xl),l(C,ue),l(ue,Rl),l(ue,x),l(x,Ge),l(Ge,bl),l(x,Il),l(x,Oe),l(Oe,Cl),l(x,Pl),l(x,Fe),l(Fe,Sl),f(e,vt,a),f(e,V,a),l(V,de),l(de,Tl),f(e,ht,a),f(e,$,a),l($,ce),l(ce,zl),f(e,_t,a),f(e,P,a),l(P,je),l(je,kl),l(P,Hl),l(P,Ze),l(Ze,ql)},p:so,d(e){e&&t(m),e&&t(R),e&&t(v),e&&t(We),e&&t(F),e&&t(Xe),e&&t(T),e&&t(Je),e&&t(E),e&&t(Ke),e&&t(z),e&&t(Qe),e&&t(b),e&&t(Ye),e&&t(k),e&&t(et),e&&t(X),e&&t(tt),e&&t(H),e&&t(lt),e&&t(p),e&&t(it),e&&t(q),e&&t(ot),e&&t(L),e&&t(rt),e&&t(M),e&&t(at),e&&t(g),e&&t(nt),e&&t(U),e&&t(st),e&&t(y),e&&t(ft),e&&t(N),e&&t(ut),e&&t(I),e&&t(dt),e&&t(D),e&&t(ct),e&&t(B),e&&t(mt),e&&t(C),e&&t(vt),e&&t(V),e&&t(ht),e&&t($),e&&t(_t),e&&t(P)}}}function co(G){let m,_;const w=[G[0],Ki];let R={$$slots:{default:[uo]},$$scope:{ctx:G}};for(let v=0;v<w.length;v+=1)R=Ol(R,w[v]);return m=new fo({props:R}),{c(){to(m.$$.fragment)},l(v){lo(m.$$.fragment,v)},m(v,h){io(m,v,h),_=!0},p(v,[h]){const O=h&1?oo(w,[h&1&&Xi(v[0]),h&0&&Xi(Ki)]):{};h&2&&(O.$$scope={dirty:h,ctx:v}),m.$set(O)},i(v){_||(ro(m.$$.fragment,v),_=!0)},o(v){ao(m.$$.fragment,v),_=!1},d(v){no(m,v)}}}const Ki={title:"Reinforcement Learning Lecture Notes",summary:"Lectures notes from David Silver's Reinforcement Learning Course",created:"2023-05-21T00:00:00.000Z",tags:["Reinforcement Learning"],toc:[{depth:1,title:"Reinforcement Learning Course Notes: David Silver’s Course",slug:"reinforcement-learning-course-notes-david-silvers-course"},{depth:1,title:"Lecture 1: Introduction to RL",slug:"lecture-1-introduction-to-rl"},{depth:3,title:"Branches of Machine learning:",slug:"branches-of-machine-learning"},{depth:3,title:"Rewards:",slug:"rewards"},{depth:3,title:"Reward Hypothesis:",slug:"reward-hypothesis"},{depth:3,title:"Sequential Decision making:",slug:"sequential-decision-making"},{depth:3,title:"Value Function:",slug:"value-function"},{depth:3,title:"Model:",slug:"model"},{depth:3,title:"Categorizing RL Agents (1):",slug:"categorizing-rl-agents-1"},{depth:3,title:"Categorizing RL Agents (2):",slug:"categorizing-rl-agents-2"},{depth:3,title:"RL Agent Taxonomy:",slug:"rl-agent-taxonomy"},{depth:3,title:"Two fundamental problems in Sequential Decision making:",slug:"two-fundamental-problems-in-sequential-decision-making"},{depth:3,title:"Exploitation and Exploration",slug:"exploitation-and-exploration"},{depth:3,title:"Prediction and Control:",slug:"prediction-and-control"}],flags:[],updated:"2023-05-21T19:58:00.330Z",slug:"/reinforcement-learning/+page.svelte.md",path:"/reinforcement-learning"};function mo(G,m,_){return G.$$set=w=>{_(0,m=Ol(Ol({},m),Ji(w)))},m=Ji(m),[m]}class _o extends Qi{constructor(m){super(),Yi(this,m,mo,co,eo,{})}}export{_o as default,Ki as metadata};
